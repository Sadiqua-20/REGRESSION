{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **REGRESSION**"
      ],
      "metadata": {
        "id": "NFbUgHS7TK_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is Simple Linear Regression?"
      ],
      "metadata": {
        "id": "b3lVGoCZTVUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 1.\n",
        "\n",
        "- Simple Linear Regression(SLR) aims to create a linear equation that predicts the value of a continuous outcome variable (y) based on the value of a single continuous predictor variable (x).\n",
        "\n",
        "- y is dependent variable and x is independent variable.\n",
        "\n",
        "- SLR attempts to determine the strength and characteristics of the relationship between y and x.\n"
      ],
      "metadata": {
        "id": "QApks2nLXFt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. What are the key assumptions of Simple Linear Regression?"
      ],
      "metadata": {
        "id": "neilGPxcTb52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 2.\n",
        "\n",
        "The key assumptions of Simple Linear Regression are :\n",
        "\n",
        "1. **Linearity :** The relationship between the independent variable (x) and dependent variable (y) should be linear.\n",
        "\n",
        "2. **Independence :**\n",
        " - Each observation should be independent of the others.\n",
        " - No duplicate or repeated measures.\n",
        "\n",
        "3. **Homoscedasticity (Constant Variance) :** The variance of the residuals should be constant across all levels of the independent variable.\n",
        "\n",
        "4. **Normality (Normal Distribution of Residuals) :** The residuals should be normally distributed.\n",
        "\n",
        "5. **No Autocorrelation :** The residuals should not be autocorrelated (i.e., not dependent on previous residuals).\n",
        "\n",
        "6. **No Outliers or High Leverage Points :** There should be no outliers or high leverage points that can influence the regression line.\n"
      ],
      "metadata": {
        "id": "JU4RZUqVnX1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. What does the coefficient m represent in the equation Y=mX+c?"
      ],
      "metadata": {
        "id": "9Ka5u6j8TiP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 3.\n",
        "\n",
        "In the equation $Y = mX + c$, $m$ represents the slope of the linear relationship between $X$ and $Y$.\n",
        "\n",
        "**Interpretation :**\n",
        "\n",
        "- $m$ indicates the change in $Y$ for a one-unit change in $X$.\n",
        "- If $m$ is positive, $Y$ increases as $X$ increases.\n",
        "- If $m$ is negative, $Y$ decreases as $X$ increases.\n",
        "- If $m$ is zero, there is no linear relationship between $X$ and $Y$.\n"
      ],
      "metadata": {
        "id": "CehuMq_so7Sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. What does the intercept c represent in the equation Y=mX+c?"
      ],
      "metadata": {
        "id": "Bn37JD3VTmSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 4.\n",
        "\n",
        "In the equation $Y = mX + c$, $c$ represents the intercept or constant term, which is the value of $Y$ when $X$ is zero.\n",
        "\n",
        "In other words, $c$ is the point at which the linear relationship crosses the $Y$-axis.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "If the equation is $Y = 2X + 3$, then $c = 3$ represents the value of $Y$ when $X$ is $0$.\n",
        "\n",
        "So, even if $X$ is $0$, $Y$ will still have a value of $3$, which is the intercept."
      ],
      "metadata": {
        "id": "478n0r4Up6W7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. How do we calculate the slope m in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "2bN_1DBTTshX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 5.\n",
        "\n",
        "To calculate the slope (m) in Simple Linear Regression, we have the formula:   \n",
        "\n",
        "$m = \\frac{Cov(X, Y)}{\\sigma^2_X}$\n",
        "\n",
        "where:\n",
        "\n",
        "- $Cov(X, Y)$ = covariance between $X$ and $Y$\n",
        "- $\\sigma^2_X$ = variance of $X$\n",
        "\n",
        "\n",
        "**Calculation Steps:**\n",
        "1. Calculate the means of $X$ (i.e., $\\overline{x}$) and $Y$ (i.e., $\\overline{y}$).\n",
        "\n",
        "2. Calculate the deviations from the means for $X$ and $Y$.\n",
        "\n",
        "3. Calculate the products of the deviations for each data point.\n",
        "\n",
        "4. Sum the products of the deviations.\n",
        "\n",
        "5. Calculate the sum of the squared deviations for $X$.\n",
        "\n",
        "6. Divide the sum of the products of the deviations by the sum of the squared deviations for $X$.\n",
        "\n",
        "The result is the slope ($m$) of the Simple Linear Regression line."
      ],
      "metadata": {
        "id": "1WjFvwjOqvZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. What is the purpose of the least squares method in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "qQW709mxTxGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 6.\n",
        "\n",
        "- The purpose of the least squares method in Simple Linear Regression is to minimize the sum of the squared errors (SSE) between observed and predicted values of the dependent variable ($y$).\n",
        "\n",
        "- The least squares method aims to find the best-fitting linear regression line that minimizes the sum of the squared differences between:\n",
        "  1. Observed values ($y$)\n",
        "  2. Predicted values ($\\hat{y}$) based on the linear regression equation"
      ],
      "metadata": {
        "id": "JKkwrsqEvKLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "7aZ_-NW4T31Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 7.\n",
        "\n",
        "In Simple Linear Regression, the coefficient of determination ($R^2$) is a measure of the proportion of variance in the dependent variable ($Y$) explained by the independent variable ($X$).\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "R² represents the percentage of the variation in $Y$ that is predictable from $X$.\n",
        "\n",
        "- $R^2 = 1$ : Perfect fit, all variance in $Y$ is explained by $X$.\n",
        "- $R^2 = 0$ : No fit, none of the variance in $Y$ is explained by $X$.\n",
        "- $0 < R^2 < 1$ : Partial fit, some variance in $Y$ is explained by $X$.\n",
        "\n",
        "**Example:**\n",
        "$R^2 = 0.7$\n",
        "\n",
        "- 70% of the variance in $Y$ is explained by $X$.\n",
        "- 30% of the variance in $Y$ remains unexplained.\n",
        "\n",
        "By interpreting R², we can evaluate the strength and usefulness of our Simple Linear Regression model."
      ],
      "metadata": {
        "id": "Pa95SGXpwf1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. What is Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "vfS3_bIoUDEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 8.\n",
        "\n",
        "- Multiple Linear Regression (MLR) is a statistical method to model the relationship between a continuous outcome variable and two or more predictor variables.\n",
        "\n",
        "- MLR aims to create a linear equation that predicts the value of a continuous outcome variable ($y$) based on the values of multiple predictor variables $x_1, x_2, x_3, ..., x_n$.\n",
        "\n",
        "- $y$ is dependent variable and $x_1, x_2, x_3, ..., x_n$ are independent variables."
      ],
      "metadata": {
        "id": "HPIbwAF-yGkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. What is the main difference between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "gPIhsqLCUFEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 9.\n",
        "\n",
        "The main difference between Simple and Multiple Linear Regression is the number of predictor variables used in the model.  \n",
        "\n",
        "**Simple Linear Regression (SLR):**\n",
        "\n",
        "- Uses only **one** predictor variable ($x$) to predict the outcome variable ($\\hat{y}$).\n",
        "- Equation: $\\hat{y} = β_0 + β_1x$\n",
        "\n",
        "**Multiple Linear Regression (MLR):**\n",
        "\n",
        "- Uses **two or more** predictor variables $(x_1, x_2, ..., x_n)$ to predict the outcome variable ($\\hat{y}$).\n",
        "- Equation: $\\hat{y} = β_0 + β_1x_1 + β_2x_2 + … + β_nx_n $\n"
      ],
      "metadata": {
        "id": "fqPPTFRXzEAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. What are the key assumptions of Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "Hs9y73JZUJOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 10.\n",
        "\n",
        "The key assumptions of Multiple Linear Regression (MLR) are :\n",
        "\n",
        "1. **Linearity :** Relationship between predictors and outcome is linear.\n",
        "\n",
        "2. **Independence :** Observations are independent.\n",
        "\n",
        "3. **Homoscedasticity :** Constant variance of residuals.\n",
        "\n",
        "4. **Normality :** Residuals are normally distributed.\n",
        "\n",
        "5. **No multicollinearity :** Predictors are not highly correlated.\n",
        "\n",
        "6. **No auto-correlation :** Residuals are not auto-correlated.\n",
        "\n",
        "7. **No Outliers or High Leverage Points :** There should be no outliers or high leverage points that can influence the regression line.\n",
        "\n"
      ],
      "metadata": {
        "id": "bxeBIw1cB57T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
      ],
      "metadata": {
        "id": "W1DrEuuSUPPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 11.\n",
        "\n",
        "**Heteroscedasticity** is a phenomenon where the variance of the residuals (or error terms) in a regression model is not constant across all levels of the predictor variables.\n",
        "In other words, the spread of the residuals changes as the predictor variables change.\n",
        "\n",
        "**Effects on Multiple Linear Regression results :**\n",
        "\n",
        "1. **Inefficient estimates :** Heteroscedasticity can lead to inefficient estimates of the regression coefficients, making it difficult to predict the outcome variable accurately.\n",
        "\n",
        "2. **Incorrect standard errors :** The standard errors of the regression coefficients may be incorrect, leading to incorrect inference and hypothesis testing.\n",
        "\n",
        "3. **Biased hypothesis tests :** Heteroscedasticity can cause biased hypothesis tests, leading to incorrect conclusions about the significance of the predictor variables.\n",
        "\n",
        "4. **Reduced model fit :*** Heteroscedasticity can reduce the overall fit of the model, making it less reliable for predictions."
      ],
      "metadata": {
        "id": "gBwHQLtzDw0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12. How can you improve a Multiple Linear Regression model with high multicollinearity?"
      ],
      "metadata": {
        "id": "D0Ivqj6YUVAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 12.\n",
        "\n",
        "To improve a Multiple Linear Regression (MLR) model with high multicollinearity:\n",
        "\n",
        "1. **Remove highly correlated predictors :** Identify predictors with high correlation (e.g., > 0.7) and remove one of them.\n",
        "\n",
        "2. **Use dimensionality reduction techniques :**\n",
        "\n",
        "3. **Use a different model :** Generalized Linear Model (GLM) can handle correlated predictors and non-normal responses.\n",
        "\n",
        "4. **Collect more data:** Increasing the sample size can help reduce the impact of multicollinearity.\n",
        "\n",
        "5. **Transform predictors:** Standardization scales predictors to have similar magnitudes, reducing multicollinearity.\n",
        "\n",
        "6. **Use a correlation matrix :** Visualize correlations between predictors to identify and address multicollinearity."
      ],
      "metadata": {
        "id": "7deqQkePFWLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q13. What are some common techniques for transforming categorical variables for use in regression models?"
      ],
      "metadata": {
        "id": "D41MYGy5UZut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS. 13\n",
        "\n",
        "Common techniques for transforming categorical variables for use in regression models:\n",
        "\n",
        "1. **One-Hot Encoding (OHE) :** Create binary columns for each category.\n",
        "\n",
        "2. **Label Encoding :** Assign numerical values to each category.\n",
        "\n",
        "3. **Dummy Variables :** Create binary columns for each category, except one reference category.\n",
        "\n",
        "4. **Effect Coding :** Similar to dummy variables, but with a different reference category.\n",
        "\n",
        "5. **Ordinal Encoding :** Assign numerical values to each category, preserving order."
      ],
      "metadata": {
        "id": "aDKCIc-KcRay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q14. What is the role of interaction terms in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "aRfOKESOUhHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 14.\n",
        "\n",
        "Interaction terms in Multiple Linear Regression:\n",
        "\n",
        "**Role :**\n",
        "1. **Capture non-additive relationships :** Interaction terms allow the effect of one independent variable to depend on the level of another independent variable.\n",
        "\n",
        "2. **Improve model fit :** Interaction terms can significantly improve the model's explanatory power and predictive accuracy.\n",
        "\n",
        "3. **Reveal hidden relationships :** Interaction terms can uncover relationships between variables that would not be apparent through main effects alone.\n",
        "\n",
        "**Interpretation:**\n",
        "1. **Synergistic effects :** Positive interaction terms indicate that the combined effect of two variables is greater than the sum of their individual effects.\n",
        "\n",
        "2. **Antagonistic effects :** Negative interaction terms indicate that the combined effect of two variables is less than the sum of their individual effects.\n"
      ],
      "metadata": {
        "id": "95oygxTzfSWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "Yx1b6DFWVJsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 15.\n",
        "\n",
        "Intercept interpretation differs between Simple and Multiple Linear Regression:\n",
        "\n",
        "**Simple Linear Regression :**\n",
        "- The intercept ($β_0$) represents the expected value of the dependent variable ($y$) when the independent variable ($x$) is equal to zero.\n",
        "\n",
        "**Multiple Linear Regression :**\n",
        "- The intercept ($β_0$) represents the expected value of the dependent variable ($y$) when all independent variables ($x_1, x_2, ..., x_n$) are equal to zero.\n",
        "\n",
        "In Multiple Linear Regression, the intercept is often not meaningful, as it's unlikely that all independent variables would be zero simultaneously."
      ],
      "metadata": {
        "id": "iRicEEi3gfKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q16. What is the significance of the slope in regression analysis, and how does it affect predictions?"
      ],
      "metadata": {
        "id": "czbqrtRAVVRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 16.\n",
        "\n",
        "\n",
        "**Significance of slope :**\n",
        "1. **Change in outcome :** The slope represents the change in the outcome variable (y) for a one-unit change in the predictor variable (x).\n",
        "\n",
        "2. **Direction of relationship :** The sign of the slope indicates the direction of the relationship between x and y.\n",
        "\n",
        "3. **Strength of relationship :** The magnitude of the slope indicates the strength of the relationship between x and y.\n",
        "\n",
        "**Effect on predictions :**\n",
        "1. **Prediction change :** A steeper slope means that small changes in x result in larger changes in predicted y values.\n",
        "\n",
        "2. **Prediction accuracy :** A slope close to zero means that changes in x have little impact on predicted y values, reducing prediction accuracy.\n",
        "\n",
        "3. **Interpretation :** The slope helps interpret the relationship between x and y, informing decisions and predictions."
      ],
      "metadata": {
        "id": "O373R-g0hUw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q17. How does the intercept in a regression model provide context for the relationship between variables?"
      ],
      "metadata": {
        "id": "n0TR7WoGVavE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 17.\n",
        "\n",
        "The intercept in a regression model provides context for the relationship between variables in the following ways:\n",
        "\n",
        "**Baseline value :**\n",
        "1. **Starting point :** The intercept represents the expected value of the dependent variable (y) when the independent variable (x) is equal to zero.\n",
        "\n",
        "2. **Baseline value :** It provides a baseline value for the dependent variable, allowing for a better understanding of the relationship between variables.\n",
        "\n",
        "**Context for slope :**\n",
        "1. **Reference point :** The intercept serves as a reference point for the slope, indicating the starting point for the relationship between variables.\n",
        "\n",
        "2. **Slope interpretation :** Knowing the intercept helps interpret the slope, as it provides context for the change in the dependent variable for a one-unit change in the independent variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "pKie5YS3tQ8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q18. What are the limitations of using R² as a sole measure of model performance?"
      ],
      "metadata": {
        "id": "mJQFV56pViGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 18.\n",
        "\n",
        "Limitations of using $R^2$ as a sole measure of model performance:\n",
        "\n",
        "1. **Overfitting :** $R^2$ can be misleadingly high for overfit models, as it only measures goodness of fit to the training data.\n",
        "\n",
        "2. **Ignoring prediction errors :** $R^2$ doesn't account for the magnitude of prediction errors, making it insensitive to outliers or large residuals.\n",
        "\n",
        "3. **Not accounting for model complexity :** $R^2$ doesn't penalize models for having too many parameters, leading to overfitting or unnecessary complexity.\n",
        "\n",
        "4. **Sensitivity to data transformations :** $R^2$ can be affected by data transformations, such as logarithmic or standardized transformations.\n",
        "\n",
        "5. **Not suitable for non-normal data :** $R^2$ assumes normality of residuals, which may not always be the case.\n",
        "\n",
        "6. **Ignores model interpretability :** $R^2$ focuses solely on predictive performance, ignoring model interpretability and the importance of coefficients."
      ],
      "metadata": {
        "id": "di5v9k-Iur4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q19. How would you interpret a large standard error for a regression coefficient?"
      ],
      "metadata": {
        "id": "tgvlNkk8Vopd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 19.\n",
        "\n",
        "A large standard error for a regression coefficient indicates:\n",
        "\n",
        "1. **High variability :** The coefficient estimate is highly variable, making it less reliable.\n",
        "\n",
        "2. **Imprecision :** The true value of the coefficient is uncertain, and the estimate may not be accurate.\n",
        "\n",
        "3. **Low confidence :** A large standard error reduces confidence in the coefficient estimate, making it harder to draw conclusions.\n",
        "\n",
        "4. **Potential multicollinearity :** Large standard errors can be a sign of multicollinearity, where predictor variables are highly correlated."
      ],
      "metadata": {
        "id": "Cr8gI3c4vtXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
      ],
      "metadata": {
        "id": "zBI9tO6qV3Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 20.\n",
        "\n",
        "Heteroscedasticity can be identified in residual plots by looking for:\n",
        "\n",
        "1. **Non-constant variance :** Residuals spread out or fan out, indicating increasing variance as the fitted values increase.\n",
        "\n",
        "2. **Non-random pattern :** Residuals display a non-random pattern, such as a cone-shaped or funnel-shaped pattern.\n",
        "\n",
        "**Why address heteroscedasticity?**\n",
        "\n",
        "1. **Biased estimates :** Heteroscedasticity can lead to biased estimates of regression coefficients.\n",
        "\n",
        "2. **Incorrect inference :** Heteroscedasticity can affect hypothesis testing and confidence intervals, leading to incorrect conclusions.\n",
        "\n",
        "3. **Poor predictions :** Heteroscedasticity can result in poor predictive performance, especially for new data."
      ],
      "metadata": {
        "id": "7soAY7AywMsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?"
      ],
      "metadata": {
        "id": "OozsI-BsV70X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 21.\n",
        "\n",
        "High $R^2$ but low adjusted $R^2$ indicates:\n",
        "\n",
        "1. **Overfitting :** The model is too complex, fitting the noise in the training data rather than the underlying pattern.\n",
        "\n",
        "2. **Too many predictors :** Including too many predictor variables, some of which may not be relevant, can inflate $R^2$ but reduce adjusted $R^2$.\n",
        "\n",
        "3. **Multicollinearity :** High correlation between predictor variables can lead to overfitting and reduce adjusted $R^2$."
      ],
      "metadata": {
        "id": "Cdn2P4rexCik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q22. Why is it important to scale variables in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "Z2VcE3O7WFHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 22.\n",
        "\n",
        "Scaling variables in Multiple Linear Regression is important for:\n",
        "\n",
        "1. **Preventing feature dominance :** Scaling ensures that variables with large ranges don't dominate the model, allowing variables with smaller ranges to contribute equally.\n",
        "\n",
        "2. **Improving model interpretability :** Scaled variables make it easier to compare coefficient estimates and understand the relative importance of each variable.\n",
        "\n",
        "3. **Enhancing numerical stability :** Scaling reduces the risk of numerical instability and overflow errors during calculations.\n",
        "\n",
        "4. **Improving convergence :** Scaling can aid in the convergence of iterative algorithms, such as gradient descent.\n",
        "\n",
        "5. **Facilitating regularization :** Scaling is essential for regularization techniques to work effectively."
      ],
      "metadata": {
        "id": "ho38kQQSxypQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q23. What is polynomial regression?"
      ],
      "metadata": {
        "id": "g8lOCvQFWLOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 23.\n",
        "\n",
        "Polynomial regression is a form of regression analysis where the relationship between the independent variable ($x$) and the dependent variable ($y$) is modeled using a polynomial equation.\n",
        "\n",
        "Equation:\n",
        "\n",
        "$y = β_0 + β_1x + β_2x^2 + … + β_nx^n$\n",
        "\n",
        "where:\n",
        "\n",
        "- $y$ = dependent variable\n",
        "- $x$ = independent variable\n",
        "- $β_0, β_1, β_2, …, β_n$ = coefficients\n",
        "- $n$ = degree of the polynomial (e.g., quadratic, cubic)"
      ],
      "metadata": {
        "id": "RgQscf-rHLWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q24. How does polynomial regression differ from linear regression?"
      ],
      "metadata": {
        "id": "uG34uiIuWQR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 24.\n",
        "\n",
        "Polynomial regression and linear regression differ in several key ways:\n",
        "\n",
        "1. **Relationship between variables :**\n",
        "- Linear Regression : Assumes a linear relationship between the independent variable (x) and the dependent variable (y).\n",
        "- Polynomial Regression : Assumes a non-linear relationship between x and y, using a polynomial equation.\n",
        "\n",
        "2. **Equation form :**\n",
        "- Linear Regression: $y = β_0 + β_1x$\n",
        "- Polynomial Regression: $y = β_0 + β_1x + β_2x² + … + β_nx^n$\n",
        "\n",
        "3. **Degree of polynomial :**\n",
        "- Linear Regression : First-degree polynomial (straight line).\n",
        "- Polynomial Regression : Higher-degree polynomial (e.g., quadratic, cubic).\n",
        "\n",
        "4. **Flexibility:**\n",
        "- Linear Regression : Limited flexibility, assumes a linear relationship.\n",
        "- Polynomial Regression : More flexible, can capture non-linear relationships.\n",
        "\n",
        "5. **Interpretation :**\n",
        "- Linear Regression : Easy to interpret, coefficients represent change in y for a one-unit change in x.\n",
        "- Polynomial Regression : More difficult to interpret, coefficients represent change in y for a one-unit change in x, but also depend on the value of x.\n",
        "\n",
        "6. **Risk of overfitting :**\n",
        "- Linear Regression : Less risk of overfitting, as the model is simpler.\n",
        "- Polynomial Regression : Higher risk of overfitting, as the model is more complex."
      ],
      "metadata": {
        "id": "ry6wl7LqH8zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q25. When is polynomial regression used?"
      ],
      "metadata": {
        "id": "fTand1PvWWfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 25.\n",
        "\n",
        "Polynomial regression is used in various situations:\n",
        "\n",
        "1. **Non-linear relationships :**\n",
        "When the relationship between the independent variable (x) and the dependent variable (y) is non-linear, polynomial regression can capture this non-linearity.\n",
        "\n",
        "2. **Curve fitting :** Polynomial regression is used for curve fitting, where the goal is to find a smooth curve that best fits the data.\n",
        "\n",
        "3. **Data with inflection points :** When the data has inflection points, polynomial regression can capture these changes in direction.\n",
        "\n",
        "4. **Modeling complex phenomena :** Polynomial regression is used to model complex phenomena, such as population growth, chemical reactions, or electrical circuits.\n",
        "\n",
        "5. **When linear regression is insufficient :** When linear regression does not provide a good fit to the data, polynomial regression can be used to improve the fit."
      ],
      "metadata": {
        "id": "_7S42LVcI8u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q26. What is the general equation for polynomial regression?"
      ],
      "metadata": {
        "id": "zNb2YERyWa1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 26.\n",
        "\n",
        "Here are the general equations for:\n",
        "\n",
        "**Simple Polynomial Regression :**\n",
        "$y = β_0 + β_1x + β_2x^2 + β_3x^3 + … + β_nx^n$\n",
        "\n",
        "**Multiple Polynomial Regression :**\n",
        "$y = β_0 + β_1x_1^1 + β_2x_1^2 + … + β_nx_1^n + β_{n+1}x_2^1 + β_{n+2}x_2^2 + … + β_mx_2^m$ + ...\n",
        "\n",
        "where:\n",
        "\n",
        "- $y$ = dependent variable\n",
        "- $x_1, x_2, …$ = independent variables\n",
        "- $β_0, β_1, β_2, …, β_n, β_{n+1}, …, β_m$ = coefficients\n",
        "- $n, m$ = degrees of the polynomials"
      ],
      "metadata": {
        "id": "AFVSCG_pKVG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q27. Can polynomial regression be applied to multiple variables?"
      ],
      "metadata": {
        "id": "Oq2f9vmjWg-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 27.\n",
        "\n",
        "Yes, polynomial regression can be applied to multiple variables. This is known as Multiple Polynomial Regression.  \n",
        "\n",
        "- In multiple polynomial regression, the relationship between the dependent variable $(y)$ and multiple independent variables $(x_1, x_2, ..., x_n)$ is modeled using polynomial equations.\n",
        "\n",
        "**Example :**\n",
        "$y = β_0 + β_1x_1^1 + β_2x_1^2 + β_3x_2^1 + β_4x_2^2 + β_5x_1x_2$  \n",
        "In this example, the model includes:\n",
        "\n",
        "- Linear and quadratic terms for $x_1$ and $x_2$\n",
        "- An interaction term between $x_1$ and $x_2$"
      ],
      "metadata": {
        "id": "Hovf5EyVMvm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q28. What are the limitations of polynomial regression?"
      ],
      "metadata": {
        "id": "5tX555n1WmA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 28.\n",
        "\n",
        "The limitations of polynomial regression are:\n",
        "\n",
        "1. **Overfitting :** High-degree polynomials can fit the noise in the data, resulting in poor generalization.\n",
        "\n",
        "2. **Multicollinearity :** Polynomial terms can be highly correlated, leading to unstable estimates.\n",
        "\n",
        "3. **Interpretability :** Polynomial models are difficult to interpret, especially with high-degree terms.\n",
        "\n",
        "4. **Computational complexity :** Fitting polynomial models can be computationally expensive, especially with large datasets.\n",
        "\n",
        "5. **Choice of degree :** Selecting the optimal degree of the polynomial can be challenging.\n",
        "\n",
        "6. **Outliers and influential points :** Polynomial regression can be sensitive to outliers and influential points.\n"
      ],
      "metadata": {
        "id": "sEhwyIkDNzgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
      ],
      "metadata": {
        "id": "U9LO2aKtWs0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 29.\n",
        "\n",
        "When selecting the degree of a polynomial, the following methods can be used to evaluate model fit:\n",
        "\n",
        "1. **Coefficient of Determination (R²) :** Measures the proportion of variance explained by the model. Higher R² values indicate better fit.\n",
        "\n",
        "2. **Mean Squared Error (MSE) :** Measures the average squared difference between predicted and actual values. Lower MSE values indicate better fit.\n",
        "\n",
        "3. **Mean Absolute Error (MAE) :** Measures the average absolute difference between predicted and actual values. Lower MAE values indicate better fit.\n",
        "\n",
        "4. **Cross-Validation :** Splits data into training and testing sets, then evaluates model performance on the testing set. Helps prevent overfitting.\n",
        "\n",
        "5. **Akaike Information Criterion (AIC) :** Measures the relative quality of a model. Lower AIC values indicate better fit.\n",
        "\n",
        "6. **Bayesian Information Criterion (BIC) :** Similar to AIC, but penalizes models with more parameters. Lower BIC values indicate better fit.\n",
        "\n",
        "7. **F-statistic :** Measures the ratio of variance explained by the model to the variance of the residuals. Higher F-statistic values indicate better fit.\n",
        "\n",
        "8. **Visual inspection :** Plotting the data and the fitted model can help identify overfitting or underfitting.\n"
      ],
      "metadata": {
        "id": "ToYajMfDOvdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q30. Why is visualization important in polynomial regression?"
      ],
      "metadata": {
        "id": "d5I67jLTWzSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 30.\n",
        "\n",
        "Visualization is important in polynomial regression to:\n",
        "\n",
        "1. **Check overfitting :** Ensure the model isn't too complex.\n",
        "\n",
        "2. **Evaluate fit :** See how well the model fits the data.\n",
        "\n",
        "3. **Identify non-linear relationships :** Detect patterns and trends.\n",
        "\n",
        "4. **Detect outliers  :** Identify influential points that affect the model.\n",
        "\n",
        "5. **Communicate results :** Clearly show findings to stakeholders."
      ],
      "metadata": {
        "id": "ex5zFF8BQUCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q31. How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "PkCJ0IywW4U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 31.\n",
        "\n",
        "Polynomial regression can be implemented in Python using the following libraries:\n",
        "\n",
        "1. NumPy: For numerical computations.\n",
        "2. SciPy: For scientific computing.\n",
        "3. Scikit-learn: For machine learning.\n",
        "\n",
        "Here's a simple example using Scikit-learn:"
      ],
      "metadata": {
        "id": "vKkXEHa6XC9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Generate sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 3, 5, 7, 11])\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform X into polynomial features\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Create linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the polynomial data\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJklT545RVVJ",
        "outputId": "19c4f5d4-e8e3-4bd9-d238-58415e6335bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.05714286  2.97142857  4.74285714  7.37142857 10.85714286]\n"
          ]
        }
      ]
    }
  ]
}